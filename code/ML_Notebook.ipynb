{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11bd3806-3b70-413b-bf00-83fc91ed25ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet datasets==2.20.0 transformers==4.49.0 tf-keras==2.17.0 accelerate==1.4.0 mlflow==2.20.2 torchvision==0.20.1 deepspeed==0.14.4\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1ec85b5-4328-40b5-aa01-a63d3c525bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql import SparkSession\n",
    "from PIL import Image\n",
    "IMAGE_RESIZE = 224\n",
    "\n",
    "landing_catalog = \"smart_claims_drv\"\n",
    "landing_schema = \"00_landing\"\n",
    "bronze_schema = \"01_bronze\"\n",
    "silver_schema = \"02_silver\"\n",
    "gold_schema = \"03_schema\"\n",
    "\n",
    "base_path = f\"/Volumes/{landing_catalog}/{landing_schema}/claims\"\n",
    "metadata_path = f\"{base_path}/autoloader_metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad3ff08-4f61-41ee-8734-e38e70cb0d1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_df = spark.table(\"smart_claims_drv.`02_silver`.training_images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7577d5a-0c6a-4732-8c04-051753997f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code resizes all images in the \"training_df\" dataset to a fixed square resolution (IMAGE_RESIZE x IMAGE_RESIZE).\n",
    "# 1. Each image is cropped to the center to ensure a square shape.\n",
    "# 2. The image is resized to the target resolution using nearest neighbor interpolation.\n",
    "# 3. The resized images are converted back to bytes and stored in a Pandas DataFrame.\n",
    "# 4. The Pandas DataFrame is converted to a Spark DataFrame and saved as a Delta table for downstream ML pipelines.\n",
    "# \n",
    "# Purpose: Standardizing image sizes ensures consistent input for machine learning models, enabling efficient training and inference.\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# --- Função de resize via pandas_udf ---\n",
    "@pandas_udf(\"binary\")\n",
    "def resize_image_udf(content_series):\n",
    "    def resize_image(content):\n",
    "        image = Image.open(io.BytesIO(content))\n",
    "        width, height = image.size\n",
    "        new_size = min(width, height)\n",
    "        # Crop central\n",
    "        image = image.crop(((width - new_size)/2, (height - new_size)/2, \n",
    "                            (width + new_size)/2, (height + new_size)/2))\n",
    "        # Resize\n",
    "        image = image.resize((IMAGE_RESIZE, IMAGE_RESIZE), Image.NEAREST)\n",
    "        # Salvar como bytes JPEG\n",
    "        output = io.BytesIO()\n",
    "        image.save(output, format='JPEG')\n",
    "        return output.getvalue()\n",
    "    return content_series.apply(resize_image)\n",
    "\n",
    "# Metadata para preview\n",
    "image_meta = {\"spark.contentAnnotation\": '{\"mimeType\": \"image/jpeg\"}'}\n",
    "\n",
    "# --- Ler o DataFrame existente ---\n",
    "training_df = spark.table(f\"{landing_catalog}.02_silver.training_images\")\n",
    "\n",
    "# --- Aplicar resize e salvar em Delta (batch) ---\n",
    "(training_df\n",
    "    .withColumn(\"content\", resize_image_udf(col(\"content\")).alias(\"content\", metadata=image_meta))\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(f\"{landing_catalog}.02_silver.training_images_resized\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f8d286-84cd-41e8-9ba3-de608fbd2ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"smart_claims_drv.02_silver.training_images_resized\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd635bcb-466e-4b60-aac3-ff4570c1d980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS smart_claims_drv.exports;\n",
    "\n",
    "CREATE VOLUME IF NOT EXISTS smart_claims_drv.exports.training_images_resized;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f613ca9-5538-4dee-97b0-3a00ff7726e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code prepares the resized images dataset for model training:\n",
    "# 1. Reads the Spark Delta table \"training_images_resized\" and converts it to a Pandas DataFrame.\n",
    "# 2. Converts the Pandas DataFrame into a Hugging Face Dataset, renaming the image column for compatibility.\n",
    "# 3. Splits the dataset into training (80%) and validation (20%) sets with a fixed seed for reproducibility.\n",
    "# 4. Sets an MLflow experiment to track model training metrics and artifacts.\n",
    "\n",
    "from datasets import Dataset\n",
    "import mlflow\n",
    "import pyarrow as pa\n",
    "\n",
    "#Setup the training experiment\n",
    "mlflow.set_experiment(\"/Users/gj.goncalvescaldas@gmail.com/image-claims-classifier\")\n",
    "\n",
    "# Spark escreve em Parquet (ou Delta → parquet under the hood)\n",
    "(\n",
    "  spark.table(\"smart_claims_drv.02_silver.training_images_resized\")\n",
    "       .write\n",
    "       .mode(\"overwrite\")\n",
    "       .parquet(\"/Volumes/smart_claims_drv/exports/training_images_resized\")\n",
    ")\n",
    "\n",
    "# Hugging Face carrega direto\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"parquet\", \n",
    "    data_files=\"/Volumes/smart_claims_drv/exports/training_images_resized/*.parquet\"\n",
    ").rename_column(\"content\", \"image\")\n",
    "\n",
    "splits = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "train_ds, val_ds = splits[\"train\"], splits[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11931f4e-4339-47c1-b1a6-7f5168f29d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code prepares image transformations for fine-tuning a pre-trained model (ResNet-50):\n",
    "# 1. Loads the model's feature extractor to get preprocessing parameters (mean, std, input size).\n",
    "# 2. Defines a transformation pipeline: bytes → PIL → Tensor → normalization.\n",
    "# 3. Creates a preprocess function that applies the transformations to a batch of images.\n",
    "# 4. Assigns the preprocessing function to the training and validation datasets, so images are transformed on-the-fly during training.\n",
    "# Purpose: Standardize and normalize images consistently with the pre-trained model, enabling effective fine-tuning.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoFeatureExtractor, AutoImageProcessor\n",
    "\n",
    "# pre-trained model from which to fine-tune\n",
    "# Check the hugging face repo for more details & models: https://huggingface.co/microsoft/resnet-50\n",
    "model_checkpoint = \"microsoft/resnet-50\"\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "from torchvision.transforms import CenterCrop, Compose, Normalize, RandomResizedCrop, Resize, ToTensor, Lambda\n",
    "\n",
    "#Extract the model feature (contains info on pre-process step required to transform our data, such as resizing & normalization)\n",
    "#Using the model parameters makes it easy to switch to another model without any change, even if the input size is different.\n",
    "model_def = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "\n",
    "#Transformations on our training dataset. we'll add some crop here\n",
    "transforms = Compose([Lambda(lambda b: Image.open(io.BytesIO(b)).convert(\"RGB\")), #byte to pil\n",
    "                        ToTensor(), #convert the PIL img to a tensor\n",
    "                        Normalize(mean=model_def.image_mean, std=model_def.image_std)\n",
    "                        ])\n",
    "\n",
    "# Add some random resiz & transformation to our training dataset\n",
    "def preprocess(batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    batch[\"image\"] = [transforms(image) for image in batch[\"image\"]]\n",
    "    return batch\n",
    "   \n",
    "#Set our training / validation transformations\n",
    "train_ds.set_transform(preprocess)\n",
    "val_ds.set_transform(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "504b8fbc-e4a7-45c8-a9b5-d4602f2712a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code prepares the model for fine-tuning on a custom image classification dataset:\n",
    "# 1. Creates a mapping between class labels and integer IDs (label2id and id2label), which Hugging Face uses for inference.\n",
    "# 2. Loads a pre-trained image classification model (ResNet-50) from the checkpoint.\n",
    "# 3. Configures the model with the correct number of classes for the dataset and the label mappings.\n",
    "# 4. The `ignore_mismatched_sizes` option allows fine-tuning even if the pre-trained model has a different number of classes.\n",
    "# Purpose: Initialize the model for fine-tuning while ensuring label consistency and compatibility with the pre-trained weights.\n",
    "\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "#Mapping between class label and value (huggingface use it during inference to output the proper label)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(set(dataset['label'])):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "    \n",
    "#Load the base model from its checkpoint\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    num_labels=len(label2id),\n",
    "    ignore_mismatched_sizes = True # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64faad7a-3c10-485c-85b8-f01af3d078c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code configures the training settings for fine-tuning the pre-trained model:\n",
    "# 1. Extracts the model name from the checkpoint path to define the output directory.\n",
    "# 2. Sets up Hugging Face TrainingArguments:\n",
    "#    - Save the fine-tuned model to a specific folder.\n",
    "#    - Run training on CPU only (no CUDA) for simplicity.\n",
    "#    - Evaluate and save checkpoints at the end of each epoch.\n",
    "#    - Train for 20 epochs and load the best model at the end.\n",
    "# Purpose: Standardize the training configuration for consistent and reproducible fine-tuning.\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    f\"/tmp/huggingface/pcb/{model_name}-finetuned\",\n",
    "    no_cuda=True, #Run on CPU for resnet to make it easier\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    num_train_epochs=20,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ac6be9a-18fc-4452-9e2b-789990d1fca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# This wrapper adds steps before and after the inference to simplify the model usage\n",
    "# Before calling the model: apply the same transform as the training, resizing the image\n",
    "# After callint the model: only keeps the main class with the probability as output\n",
    "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        # instantiate model in evaluation mode\n",
    "        self.pipeline.model.eval()\n",
    "\n",
    "    def predict(self, context, images):\n",
    "        from PIL import Image\n",
    "        with torch.set_grad_enabled(False):\n",
    "            #Convert the byte to PIL images\n",
    "            images = images['content'].apply(lambda b: Image.open(io.BytesIO(b))).to_list()\n",
    "            #the pipeline returns the probability for all the class\n",
    "            predictions = self.pipeline.predict(images)\n",
    "            #Filter & returns only the class with the highest score [{'score': 0.999038815498352, 'label': 'normal'}, ...]\n",
    "            return pd.DataFrame([max(r, key=lambda x: x['score']) for r in predictions])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5d3f8c-3633-4b8f-af9d-8f9cd589a516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, DefaultDataCollator, EarlyStoppingCallback\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "with mlflow.start_run(run_name=\"hugging_face_new\") as run:\n",
    "    mlflow.log_input(mlflow.data.from_huggingface(train_ds, \"training\"))\n",
    "\n",
    "    # use real class count instead of 3\n",
    "    def collate_fn(examples):\n",
    "        import torch\n",
    "        pixel_values = torch.stack([e[\"image\"] for e in examples])\n",
    "        labels = torch.tensor([label2id[e[\"label\"]] for e in examples], dtype=torch.long)\n",
    "        labels = torch.nn.functional.one_hot(labels, num_classes=len(label2id)).float()\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "    trainer = Trainer(model, args, train_dataset=train_ds, eval_dataset=val_ds, tokenizer=model_def, data_collator=collate_fn)\n",
    "    train_results = trainer.train()\n",
    "\n",
    "    # Build final HF pipeline\n",
    "    classifier = pipeline(\"image-classification\", model=trainer.state.best_model_checkpoint, tokenizer=model_def)\n",
    "\n",
    "    # ---- moved from your Cell B, so it's inside the SAME run ----\n",
    "    import pandas as pd\n",
    "    wrapped_model = ModelWrapper(classifier)\n",
    "    test_df = spark.table(\"smart_claims_drv.02_silver.training_images_resized\").select('content').toPandas()\n",
    "    predictions = wrapped_model.predict(None, test_df)\n",
    "    signature = infer_signature(test_df, predictions)\n",
    "\n",
    "    reqs = mlflow.transformers.get_default_pip_requirements(model)\n",
    "\n",
    "    # LOG the model and CAPTURE the URI\n",
    "    logged = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=wrapped_model,\n",
    "        pip_requirements=reqs,\n",
    "        signature=signature,\n",
    "    )\n",
    "\n",
    "# keep these prints to sanity-check\n",
    "from mlflow import artifacts\n",
    "print(\"logged.model_uri:\", logged.model_uri)   # e.g., runs://model\n",
    "print(\"logged.run_id  :\", logged.run_id)\n",
    "print(\"model files    :\", artifacts.list_artifacts(logged.model_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a53ccb-763c-43ae-89ee-343a2e66c7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = \"smart_claims_drv.03_gold.claims_damage_level\"\n",
    "\n",
    "registered = mlflow.register_model(\n",
    "    model_uri=logged.model_uri,\n",
    "    name=model_name,\n",
    ")\n",
    "\n",
    "MlflowClient().set_registered_model_alias(\n",
    "    name=model_name,\n",
    "    alias=\"prod\",\n",
    "    version=registered.version,\n",
    ")\n",
    "\n",
    "print(f\"Registered {model_name} v{registered.version} and set alias 'prod'.\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06592612-fe1f-4e8c-b63c-ed62d846009c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = predictions.selectExpr(\"path\", \"label\", \"damage_prediction.label as predictions\", \"damage_prediction.score as score\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c41e3b0-59be-4692-b5c4-5e4cf63a207d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# create confusion matrix\n",
    "confusion_matrix = pd.crosstab(results['label'], results['predictions'])\n",
    "\n",
    "# plot confusion matrix\n",
    "fig = plt.figure()\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd1ef50-e15a-404c-8bbe-75c3f0d497eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_images = (spark.read.table(\"smart_claims_drv.02_silver.claim_images\")\n",
    "                   .withColumn(\"damage_prediction\", predict_damage_udf(*columns)))\n",
    "\n",
    "metadata = spark.table(\"smart_claims_drv.01_bronze.claim_images_meta\")\n",
    "\n",
    "raw_images.join(metadata, on=\"image_name\").write.mode('overwrite').saveAsTable(\"smart_claims_drv.03_gold.claim_images_predicted\")\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5636923071439555,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ML_Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
